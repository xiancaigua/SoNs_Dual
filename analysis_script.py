import json
import numpy as np
import matplotlib.pyplot as plt
import os
from collections import defaultdict
import re

# =======================================================
# ğŸ“Œ é…ç½®åŒº
# =======================================================
# å®éªŒæ–‡ä»¶æ‰€åœ¨çš„ç›®å½• (å‡è®¾å’Œè„šæœ¬åœ¨åŒä¸€ç›®å½•ä¸‹)
DATA_DIRECTORY = "base1\simulation_results" 
TOTAL_EXPERIMENTS = 600
EXPERIMENTS_PER_SCENE = 100
SCENE_COUNT = TOTAL_EXPERIMENTS // EXPERIMENTS_PER_SCENE

# =======================================================
# âš™ï¸ æ ¸å¿ƒæ•°æ®å¤„ç†å‡½æ•°
# =======================================================

def calculate_single_experiment_metrics(data):
    """è®¡ç®—å•ä¸ªå®éªŒçš„å…³é”®æŒ‡æ ‡."""
    stats = data['statistics']
    details = data['agent_details']
    
    # 1. æˆåŠŸç‡ (å¸ƒå°”å€¼è½¬æ¢ä¸ºæ•°å­—)
    success_rate = 1.0 if stats['victim_rescued'] else 0.0

    # 2. è½¨è¿¹é•¿åº¦æ€»å’Œ
    # æ³¨æ„ï¼šéœ€è¦å¤„ç† 'total_small_agents' å¯èƒ½å°äºå®é™…å­˜æ´»æœºå™¨äººçš„æƒ…å†µï¼ˆå¦‚å¢æ´ï¼‰ï¼Œ
    # ä½†æ­¤å¤„æˆ‘ä»¬åªè®¡ç®—å­˜åœ¨çš„è½¨è¿¹é•¿åº¦ã€‚
    total_trajectory_length = sum(
        a['trajectory_length'] for a in details['small_agents'] if 'trajectory_length' in a
    ) + sum(
        a['trajectory_length'] for a in details['large_agents'] if 'trajectory_length' in a
    )

    # 3. èƒ½é‡æ¶ˆè€—æ€»å’Œ
    total_energy_cost = sum(
        a['energycost'] for a in details['small_agents'] if 'energycost' in a
    ) + sum(
        a['energycost'] for a in details['large_agents'] if 'energycost' in a
    )
    
    # 4. å°æœºå™¨äºº/å¤§æœºå™¨äººå•ç‹¬çš„èƒ½é‡æ¶ˆè€—åˆ—è¡¨
    small_agent_costs = [a['energycost'] for a in details['small_agents'] if 'energycost' in a]
    large_agent_costs = [a['energycost'] for a in details['large_agents'] if 'energycost' in a]

    return {
        'simulation_duration': stats['simulation_duration'],
        'success_rate': success_rate,
        'dead_agents': stats['dead_agents'],
        'coverage_percentage': stats['coverage_percentage'],
        'total_trajectory_length': total_trajectory_length,
        'total_energy_cost': total_energy_cost,
        'small_agent_costs': small_agent_costs,
        'large_agent_costs': large_agent_costs
    }

def aggregate_statistics():
    """
    å¯¹æ‰€æœ‰å®éªŒæ•°æ®è¿›è¡Œåˆ†ç»„å’Œç»Ÿè®¡è®¡ç®—ã€‚
    """
    
    # --- ç¬¬ä¸€æ­¥ï¼šè·å–æ–‡ä»¶åˆ—è¡¨å¹¶æ’åº ---
    all_files = []
    for filename in os.listdir(DATA_DIRECTORY):
        # åªå¤„ç† json æ–‡ä»¶ï¼Œå¹¶æ’é™¤ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶
        if filename.endswith('.json') and not filename.startswith('analysis_report'):
            all_files.append(filename)

    # ğŸš¨ å…³é”®ï¼šæŒ‰æ–‡ä»¶åæ’åº (å‡è®¾æ—¶é—´æˆ³æ’åºæ˜¯æ­£ç¡®çš„å®éªŒé¡ºåº)
    all_files.sort()
    
    if len(all_files) < TOTAL_EXPERIMENTS:
        print(f"è­¦å‘Š: ç›®å½•ä¸‹åªæ‰¾åˆ° {len(all_files)} ä¸ª JSON æ–‡ä»¶ï¼Œä½†é¢„æœŸä¸º {TOTAL_EXPERIMENTS} ä¸ªã€‚å°†ä½¿ç”¨æ‰¾åˆ°çš„æ–‡ä»¶è¿›è¡Œåˆ†æã€‚")
        
    files_to_process = all_files[:TOTAL_EXPERIMENTS]
    
    # --- ç¬¬äºŒæ­¥ï¼šåˆ†ç»„å’Œè®¡ç®— ---
    scene_results = defaultdict(lambda: defaultdict(list))
    
    for i, file_name in enumerate(files_to_process):
        # i ä» 0 å¼€å§‹ï¼Œæ–‡ä»¶è®¡æ•°ä» 1 å¼€å§‹
        exp_index = i + 1 
        scene_id = i // EXPERIMENTS_PER_SCENE + 1
        
        full_path = os.path.join(DATA_DIRECTORY, file_name)
        
        try:
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except json.JSONDecodeError:
            print(f"è­¦å‘Š: æ–‡ä»¶ {file_name} JSON è§£æå¤±è´¥ï¼Œè·³è¿‡ã€‚")
            continue
        except Exception as e:
            print(f"è­¦å‘Š: è¯»å–æ–‡ä»¶ {file_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue

        metrics = calculate_single_experiment_metrics(data)

        # è®°å½•æ¯é¡¹æŒ‡æ ‡
        scene_results[scene_id]['simulation_duration'].append(metrics['simulation_duration'])
        scene_results[scene_id]['success_rate'].append(metrics['success_rate'])
        scene_results[scene_id]['dead_agents'].append(metrics['dead_agents'])
        scene_results[scene_id]['coverage_percentage'].append(metrics['coverage_percentage'])
        scene_results[scene_id]['total_trajectory_length'].append(metrics['total_trajectory_length'])
        scene_results[scene_id]['total_energy_cost'].append(metrics['total_energy_cost'])
        scene_results[scene_id]['small_agent_costs_flat'].extend(metrics['small_agent_costs'])
        scene_results[scene_id]['large_agent_costs_flat'].extend(metrics['large_agent_costs'])
        
        # è®°å½•æ¯æ¬¡å®éªŒçš„æœºå™¨äººä¸ªä½“æ¶ˆè€—æ€»å’Œ (ç”¨äºè®¡ç®—å¹³å‡å€¼)
        # æ³¨æ„ï¼šä½¿ç”¨ np.mean æ—¶è¦æ£€æŸ¥åˆ—è¡¨æ˜¯å¦ä¸ºç©º
        scene_results[scene_id]['exp_small_cost_avg'].append(np.mean(metrics['small_agent_costs']) if metrics['small_agent_costs'] else 0)
        scene_results[scene_id]['exp_large_cost_avg'].append(np.mean(metrics['large_agent_costs']) if metrics['large_agent_costs'] else 0)

    # --- ç¬¬ä¸‰æ­¥ï¼šæœ€ç»ˆæ±‡æ€»è®¡ç®— ---
    final_global_summary = {}
    final_scene_summary = {}
    
    all_metrics = defaultdict(list)
    
    # åœºæ™¯ ID åˆ—è¡¨
    actual_scene_ids = sorted(scene_results.keys())

    for scene_id in actual_scene_ids:
        results = scene_results[scene_id]
        scene_summary = {}
        
        # 1. æ ¸å¿ƒæŒ‡æ ‡ (å¹³å‡å€¼ï¼Œç”¨äºå…¨å±€å’Œåœºæ™¯æ±‡æ€»)
        core_metrics = {
            'simulation_duration': np.array(results['simulation_duration']),
            'success_rate': np.array(results['success_rate']),
            'dead_agents': np.array(results['dead_agents']),
            'coverage_percentage': np.array(results['coverage_percentage']),
            'total_trajectory_length': np.array(results['total_trajectory_length']),
            'total_energy_cost': np.array(results['total_energy_cost']),
        }

        # åœºæ™¯æ±‡æ€» (åŒ…å«å¹³å‡å€¼å’Œæ ‡å‡†å·®)
        for name, values in core_metrics.items():
            if values.size == 0: continue
            scene_summary[f'avg_{name}'] = np.mean(values)
            scene_summary[f'std_{name}'] = np.std(values)
            all_metrics[name].extend(values) # æ”¶é›†æ‰€æœ‰åœºæ™¯æ•°æ®ç”¨äºå…¨å±€è®¡ç®—
            
        # 2. æœºå™¨äººä¸ªä½“èƒ½è€—è¯¦ç»†åˆ†æ
        
        # å°æœºå™¨äººèƒ½è€—
        small_costs = np.array(results['exp_small_cost_avg'])
        scene_summary['small_cost_avg_of_exp_avg'] = np.mean(small_costs) if small_costs.size > 0 else 0
        scene_summary['small_cost_std_of_exp_avg'] = np.std(small_costs) if small_costs.size > 0 else 0
        
        flat_small_costs = np.array(results['small_agent_costs_flat'])
        if flat_small_costs.size > 0:
            scene_summary['small_cost_min'] = np.min(flat_small_costs)
            scene_summary['small_cost_max'] = np.max(flat_small_costs)
            scene_summary['small_cost_variance'] = np.var(flat_small_costs)
        else:
            scene_summary['small_cost_min'] = scene_summary['small_cost_max'] = scene_summary['small_cost_variance'] = 0

        # å¤§æœºå™¨äººèƒ½è€—
        large_costs = np.array(results['exp_large_cost_avg'])
        scene_summary['large_cost_avg_of_exp_avg'] = np.mean(large_costs) if large_costs.size > 0 else 0
        scene_summary['large_cost_std_of_exp_avg'] = np.std(large_costs) if large_costs.size > 0 else 0
        
        flat_large_costs = np.array(results['large_agent_costs_flat'])
        if flat_large_costs.size > 0:
            scene_summary['large_cost_min'] = np.min(flat_large_costs)
            scene_summary['large_cost_max'] = np.max(flat_large_costs)
            scene_summary['large_cost_variance'] = np.var(flat_large_costs)
        else:
             scene_summary['large_cost_min'] = scene_summary['large_cost_max'] = scene_summary['large_cost_variance'] = 0
        
        final_scene_summary[f'scene_{scene_id}'] = scene_summary

    # 3. å…¨å±€æ±‡æ€» (è®¡ç®—æ‰€æœ‰åœºæ™¯çš„å¹³å‡å€¼)
    for name, values in all_metrics.items():
        values_array = np.array(values)
        if values_array.size > 0:
            final_global_summary[f'global_avg_{name}'] = np.mean(values_array)
            final_global_summary[f'global_std_{name}'] = np.std(values_array)
        else:
            final_global_summary[f'global_avg_{name}'] = 0
            final_global_summary[f'global_std_{name}'] = 0
            
    return final_global_summary, final_scene_summary

def save_json_report(data, filename):
    """ä¿å­˜JSONæŠ¥å‘Šåˆ°æ–‡ä»¶."""
    with open(filename, 'w', encoding='utf-8') as f:
        # np.float64 ç­‰ç±»å‹ä¸èƒ½ç›´æ¥åºåˆ—åŒ–ï¼Œéœ€è¦è½¬æ¢ä¸º list æˆ– float
        json.dump(data, f, indent=4, ensure_ascii=False, default=lambda x: x.tolist() if isinstance(x, np.ndarray) else x)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {filename}")

# ç»˜å›¾é…ç½®æ¥å£
PLOT_CONFIG = {
    "font_size": 12,
    "bar_color": 'skyblue',
    "error_color": 'dimgray',
    "error_capsize": 5,
    "alpha": 0.7
}

# =======================================================
# âš™ï¸ æ ¸å¿ƒæ•°æ®å¤„ç†å‡½æ•° (ä¿æŒä¸å˜)
# =======================================================

# ... (aggregate_statistics ç­‰å‡½æ•°ä¿æŒä¸å˜) ...

# =======================================================
# ğŸ“ˆ å¯è§†åŒ–å‡½æ•° (plot_scene_comparisons)
# =======================================================

def plot_scene_comparisons(scene_summary, output_dir="analysis_plots"):
    """
    Generates comparison charts for metrics across different scenes.
    
    Args:
        scene_summary (dict): Detailed statistics aggregated by scene.
        output_dir (str): Directory to save the plots.
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract actual scene IDs
    scene_ids_str = sorted(scene_summary.keys())
    # Use list comprehension to handle case where no scenes are present
    if not scene_ids_str:
        print("No scenes found for plotting.")
        return
    
    scene_ids_int = [int(s.split('_')[-1]) for s in scene_ids_str]
    
    # English Titles for plotting
    metrics_to_plot = {
        'simulation_duration': 'Avg. Simulation Duration (s)',
        'success_rate': 'Success Rate',
        'dead_agents': 'Avg. Robot Deaths',
        'coverage_percentage': 'Avg. Map Coverage (%)',
        'total_trajectory_length': 'Avg. Total Trajectory Length',
        'total_energy_cost': 'Avg. Total Energy Consumption',
        'small_cost_avg_of_exp_avg': 'Avg. Small Agent Energy Cost',
        'large_cost_avg_of_exp_avg': 'Avg. Large Agent Energy Cost',
    }
    
    for metric, title in metrics_to_plot.items():
        # Determine the dictionary keys for average and standard deviation
        if metric.startswith(('small', 'large')):
            avg_key = metric
            std_key = metric.replace('avg', 'std')
        else:
            avg_key = f'avg_{metric}'
            std_key = f'std_{metric}'

        averages = [scene_summary[s].get(avg_key, 0.0) for s in scene_ids_str]
        stds = [scene_summary[s].get(std_key, 0.0) for s in scene_ids_str]
        
        # --- è§£å†³ ValueError: 'yerr' must not contain None çš„å…³é”®ä¿®å¤ ---
        # ç¡®ä¿æ‰€æœ‰æ ‡å‡†å·®å€¼éƒ½æ˜¯æœ‰æ•ˆçš„æµ®ç‚¹æ•°æˆ–0ï¼Œè€Œä¸æ˜¯ None/NaNã€‚
        # è¿™é‡Œä½¿ç”¨ np.nan_to_num å°†æ‰€æœ‰ NaN/Inf è½¬æ¢ä¸º 0ã€‚
        stds_cleaned = np.nan_to_num(stds, nan=0.0, posinf=0.0, neginf=0.0)
        
        # Check if the data is all zero or flat
        if not any(averages):
             print(f"Skipping plot for '{metric}': All average values are zero.")
             continue
        
        # Set Matplotlib font size configuration
        plt.rcParams.update({'font.size': PLOT_CONFIG["font_size"]})

        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Use error bars (Standard Deviation)
        ax.bar(scene_ids_int, averages, 
               yerr=stds_cleaned, # ä½¿ç”¨æ¸…ç†åçš„æ ‡å‡†å·®åˆ—è¡¨
               capsize=PLOT_CONFIG["error_capsize"], 
               color=PLOT_CONFIG["bar_color"], 
               alpha=PLOT_CONFIG["alpha"], 
               edgecolor='black', # Changed to black for better contrast
               error_kw={'ecolor': PLOT_CONFIG["error_color"], 'linewidth': 1}) # Specific error bar properties

        ax.set_xlabel("Scene ID")
        ax.set_ylabel(title)
        ax.set_title(f"Comparison of {title} Across Scenes")
        ax.set_xticks(scene_ids_int)
        ax.grid(axis='y', linestyle='--', alpha=0.6)
        
        # Add labels for mean and standard deviation
        max_avg = max(averages) if averages else 1.0 # Avoid division by zero
        
        for i, avg in enumerate(averages):
            std_val = stds_cleaned[i]
            
            # Label position calculation: above the bar height + error bar + a small offset
            # Add a safety margin to the vertical position
            offset = max_avg * 0.015
            label_pos = avg + std_val + offset
            
            ax.text(scene_ids_int[i], label_pos, 
                    f'{avg:.2f} Â± {std_val:.2f}', 
                    ha='center', va='bottom', fontsize=PLOT_CONFIG["font_size"] - 2)

        plt.tight_layout()
        # Clean file name to avoid path issues
        safe_metric_name = re.sub(r'[^\w\-_\. ]', '_', metric)
        plot_filename = os.path.join(output_dir, f'{safe_metric_name}_comparison.png')
        plt.savefig(plot_filename)
        plt.close(fig)
        print(f"âœ… Plot saved: {plot_filename}")


def plot_scene_comparisons_no_error(scene_summary, output_dir="analysis_plots"):
    """
    Generates comparison bar charts for metrics across different scenes, 
    EXCLUDING error bars (standard deviation/variance).
    
    Args:
        scene_summary (dict): Detailed statistics aggregated by scene.
        output_dir (str): Directory to save the plots.
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # Extract actual scene IDs
    scene_ids_str = sorted(scene_summary.keys())
    if not scene_ids_str:
        print("No scenes found for plotting.")
        return
    
    scene_ids_int = [int(s.split('_')[-1]) for s in scene_ids_str]
    
    # English Titles for plotting (reused from plot_scene_comparisons)
    metrics_to_plot = {
        'simulation_duration': 'Avg. Simulation Duration (s)',
        'success_rate': 'Success Rate',
        'dead_agents': 'Avg. Robot Deaths',
        'coverage_percentage': 'Avg. Map Coverage (%)',
        'total_trajectory_length': 'Avg. Total Trajectory Length',
        'total_energy_cost': 'Avg. Total Energy Consumption',
        'small_cost_avg_of_exp_avg': 'Avg. Small Agent Energy Cost',
        'large_cost_avg_of_exp_avg': 'Avg. Large Agent Energy Cost',
    }
    
    for metric, title in metrics_to_plot.items():
        # Determine the dictionary keys for average
        if metric.startswith(('small', 'large')):
            avg_key = metric
        else:
            avg_key = f'avg_{metric}'

        # è·å–å¹³å‡å€¼
        averages = [scene_summary[s].get(avg_key, 0.0) for s in scene_ids_str]
        
        # Check if the data is all zero or flat
        if not any(averages):
             print(f"Skipping plot for '{metric}' (No Error Bar version): All average values are zero.")
             continue
        
        # Set Matplotlib font size configuration
        plt.rcParams.update({'font.size': PLOT_CONFIG["font_size"]})

        fig, ax = plt.subplots(figsize=(10, 6))
        
        # æ ¸å¿ƒæ”¹åŠ¨ï¼šä¸ä½¿ç”¨ yerr å‚æ•°
        ax.bar(scene_ids_int, averages, 
               color=PLOT_CONFIG["bar_color"], 
               alpha=PLOT_CONFIG["alpha"], 
               edgecolor='black') 

        ax.set_xlabel("Scene ID")
        ax.set_ylabel(title)
        ax.set_title(f"Comparison of {title} Across Scenes (No Error Bar)") # æ›´æ”¹æ ‡é¢˜ä»¥åŒºåˆ†
        ax.set_xticks(scene_ids_int)
        ax.grid(axis='y', linestyle='--', alpha=0.6)
        
        # Add labels for mean value only
        max_avg = max(averages) if averages else 1.0 
        
        for i, avg in enumerate(averages):
            # æ ‡ç­¾ä½ç½®ï¼šæ¡å½¢å›¾é¡¶éƒ¨åŠ ä¸Šä¸€ä¸ªå°çš„å›ºå®šåç§»é‡
            offset = max_avg * 0.015
            label_pos = avg + offset
            
            # æ ¸å¿ƒæ”¹åŠ¨ï¼šç®€åŒ–æ ‡ç­¾å†…å®¹ï¼Œåªæ˜¾ç¤ºå¹³å‡å€¼
            ax.text(scene_ids_int[i], label_pos, 
                    f'{avg:.2f}', 
                    ha='center', va='bottom', fontsize=PLOT_CONFIG["font_size"] - 2)

        plt.tight_layout()
        # Clean file name and add '_no_error' suffix
        safe_metric_name = re.sub(r'[^\w\-_\. ]', '_', metric)
        plot_filename = os.path.join(output_dir, f'{safe_metric_name}_no_error_comparison.png')
        plt.savefig(plot_filename)
        plt.close(fig)
        print(f"âœ… Plot saved (No Error Bar): {plot_filename}")

if __name__ == "__main__":
    print(f"--- å¯åŠ¨å®éªŒç»“æœåˆ†æ (é¢„æœŸ {TOTAL_EXPERIMENTS} æ¬¡å®éªŒ, æ¯ {EXPERIMENTS_PER_SCENE} æ¬¡ä¸ºä¸€ç»„) ---")
    
    # 1. èšåˆç»Ÿè®¡æ•°æ®
    global_summary, scene_summary = aggregate_statistics() 
    
    if not scene_summary:
        print("è‡´å‘½é”™è¯¯: æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•å®éªŒæ–‡ä»¶ã€‚è¯·æ£€æŸ¥ DATA_DIRECTORY æ˜¯å¦æ­£ç¡®ã€‚")
    else:
        # 2. è¾“å‡º (1) å…¨å±€æ±‡æ€» JSON
        print("\n--- æ­£åœ¨ç”Ÿæˆå…¨å±€æ±‡æ€»æŠ¥å‘Š ---")
        save_json_report(global_summary, "analysis_report_global_summary.json")

        # 3. è¾“å‡º (2) åœºæ™¯è¯¦ç»† JSON æŠ¥å‘Š
        print("\n--- æ­£åœ¨ç”Ÿæˆåœºæ™¯è¯¦ç»†æŠ¥å‘Š ---")
        save_json_report(scene_summary, "analysis_report_scene_details.json")

        # 4. ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        print("\n--- æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ ---")
        plot_scene_comparisons(scene_summary)

        plot_scene_comparisons_no_error(scene_summary)
        
        print("\n--- åˆ†æå®Œæˆ ---")